07-25 16:44:59 method: DA
07-25 16:44:59 model_name: VareFea
07-25 16:44:59 data_name: Battery
07-25 16:44:59 data_dir: ../processed
07-25 16:44:59 transfer_task: [[1], [2]]
07-25 16:44:59 normlizetype: mean-std
07-25 16:44:59 adabn: False
07-25 16:44:59 eval_all: False
07-25 16:44:59 adabn_epochs: 3
07-25 16:44:59 cuda_device: 0
07-25 16:44:59 checkpoint_dir: ./checkpoint
07-25 16:44:59 pretrained: False
07-25 16:44:59 batch_size: 128
07-25 16:44:59 num_workers: 0
07-25 16:44:59 patience: 50
07-25 16:44:59 bottleneck: True
07-25 16:44:59 bottleneck_num: 128
07-25 16:44:59 last_batch: False
07-25 16:44:59 number_of_features: 16
07-25 16:44:59 sequence_length: 256
07-25 16:44:59 hidden_layer_depth: 5
07-25 16:44:59 latent_length: 20
07-25 16:44:59 block: GRU
07-25 16:44:59 rnn_hidden_size: 32
07-25 16:44:59 distance_metric: False
07-25 16:44:59 distance_loss: JMMD
07-25 16:44:59 trade_off_distance: Step
07-25 16:44:59 lam_distance: 1.2
07-25 16:44:59 domain_adversarial: True
07-25 16:44:59 adversarial_loss: CDA
07-25 16:44:59 hidden_size: 1024
07-25 16:44:59 trade_off_adversarial: Step
07-25 16:44:59 lam_adversarial: 2
07-25 16:44:59 opt: adam
07-25 16:44:59 lr: 0.001
07-25 16:44:59 momentum: 0.9
07-25 16:44:59 weight_decay: 1e-05
07-25 16:44:59 lr_scheduler: step
07-25 16:44:59 gamma: 0.8
07-25 16:44:59 steps: 80, 95, 105
07-25 16:44:59 criterion: CeLoss
07-25 16:44:59 middle_epoch: 70
07-25 16:44:59 middle_vae_epoch: 256
07-25 16:44:59 max_epoch: 128
07-25 16:44:59 print_step: 600
07-25 16:44:59 using 1 gpus



100%|██████████| 5/5 [00:12<00:00,  2.40s/it]





100%|██████████| 5/5 [00:12<00:00,  2.49s/it]
07-25 16:45:28 Refactor network
07-25 16:45:28 ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [128, 16, 256]            --
├─Encoder: 1-1                           [128, 32]                 --
│    └─GRU: 2-1                          [256, 128, 64]            84,864
│    └─Sequential: 2-2                   [128, 1]                  --
│    │    └─Linear: 3-1                  [128, 180]                5,940
│    │    └─ReLU: 3-2                    [128, 180]                --
│    │    └─Linear: 3-3                  [128, 1]                  180
│    │    └─Softmax: 3-4                 [128, 1]                  --
├─Lambda: 1-2                            [128, 20]                 --
│    └─Linear: 2-3                       [128, 20]                 660
│    └─Linear: 2-4                       [128, 20]                 660
├─Decoder: 1-3                           [128, 16, 256]            --
│    └─Linear: 2-5                       [128, 32]                 672
│    └─GRU: 2-6                          [256, 128, 32]            28,704
│    └─Linear: 2-7                       [256, 128, 16]            528
==========================================================================================
Total params: 122,208
Trainable params: 122,208
Non-trainable params: 0
Total mult-adds (G): 3.72
==========================================================================================
Input size (MB): 2.10
Forward/backward pass size (MB): 29.62
Params size (MB): 0.49
Estimated Total Size (MB): 32.21
==========================================================================================
07-25 16:45:28 Prediction network
07-25 16:45:28 ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [128, 5]                  --
├─Encoder: 1-1                           [128, 32]                 --
│    └─GRU: 2-1                          [256, 128, 64]            84,864
│    └─Sequential: 2-2                   [128, 1]                  --
│    │    └─Linear: 3-1                  [128, 180]                5,940
│    │    └─ReLU: 3-2                    [128, 180]                --
│    │    └─Linear: 3-3                  [128, 1]                  180
│    │    └─Softmax: 3-4                 [128, 1]                  --
├─Lambda: 1-2                            [128, 20]                 --
│    └─Linear: 2-3                       [128, 20]                 660
│    └─Linear: 2-4                       [128, 20]                 660
├─Linear: 1-3                            [128, 128]                --
│    └─Linear: 2-5                       [128, 64]                 1,344
│    └─Linear: 2-6                       [128, 128]                8,320
├─Sequential: 1-4                        [128, 128]                --
│    └─Linear: 2-7                       [128, 128]                16,512
│    └─ReLU: 2-8                         [128, 128]                --
│    └─Dropout: 2-9                      [128, 128]                --
├─Linear: 1-5                            [128, 5]                  645
==========================================================================================
Total params: 119,125
Trainable params: 119,125
Non-trainable params: 0
Total mult-adds (G): 2.79
==========================================================================================
Input size (MB): 2.10
Forward/backward pass size (MB): 17.34
Params size (MB): 0.48
Estimated Total Size (MB): 19.91
==========================================================================================
07-25 16:45:28 -----Epoch 0/255-----
07-25 16:45:28 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
07-25 16:45:29 Epoch: 0 [0/14701], Train Loss: 260360.5312249.6 examples/sec 0.51 sec/batch
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [128, 16, 256]            --
├─Encoder: 1-1                           [128, 32]                 --
│    └─GRU: 2-1                          [256, 128, 64]            84,864
│    └─Sequential: 2-2                   [128, 1]                  --
│    │    └─Linear: 3-1                  [128, 180]                5,940
│    │    └─ReLU: 3-2                    [128, 180]                --
│    │    └─Linear: 3-3                  [128, 1]                  180
│    │    └─Softmax: 3-4                 [128, 1]                  --
├─Lambda: 1-2                            [128, 20]                 --
│    └─Linear: 2-3                       [128, 20]                 660
│    └─Linear: 2-4                       [128, 20]                 660
├─Decoder: 1-3                           [128, 16, 256]            --
│    └─Linear: 2-5                       [128, 32]                 672
│    └─GRU: 2-6                          [256, 128, 32]            28,704
│    └─Linear: 2-7                       [256, 128, 16]            528
==========================================================================================
Total params: 122,208
Trainable params: 122,208
Non-trainable params: 0
Total mult-adds (G): 3.72
==========================================================================================
Input size (MB): 2.10
Forward/backward pass size (MB): 29.62
Params size (MB): 0.49
Estimated Total Size (MB): 32.21
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [128, 5]                  --
├─Encoder: 1-1                           [128, 32]                 --
│    └─GRU: 2-1                          [256, 128, 64]            84,864
│    └─Sequential: 2-2                   [128, 1]                  --
│    │    └─Linear: 3-1                  [128, 180]                5,940
│    │    └─ReLU: 3-2                    [128, 180]                --
│    │    └─Linear: 3-3                  [128, 1]                  180
│    │    └─Softmax: 3-4                 [128, 1]                  --
├─Lambda: 1-2                            [128, 20]                 --
│    └─Linear: 2-3                       [128, 20]                 660
│    └─Linear: 2-4                       [128, 20]                 660
├─Linear: 1-3                            [128, 128]                --
│    └─Linear: 2-5                       [128, 64]                 1,344
│    └─Linear: 2-6                       [128, 128]                8,320
├─Sequential: 1-4                        [128, 128]                --
│    └─Linear: 2-7                       [128, 128]                16,512
│    └─ReLU: 2-8                         [128, 128]                --
│    └─Dropout: 2-9                      [128, 128]                --
├─Linear: 1-5                            [128, 5]                  645
==========================================================================================
Total params: 119,125
Trainable params: 119,125
Non-trainable params: 0
Total mult-adds (G): 2.79
==========================================================================================
Input size (MB): 2.10
Forward/backward pass size (MB): 17.34
Params size (MB): 0.48
Estimated Total Size (MB): 19.91
==========================================================================================
Model build successfully!
Training Variational Autoencoder with minimum loss
07-25 16:45:33 Epoch: 0 source_train-Loss: 37008.4404, recon_loss:15699.5557, kl_loss:1.0332, Cost 5.2998 sec
07-25 16:45:34 Epoch: 0 source_val-Loss: 15091.7961, recon_loss:15329.3594, kl_loss:1.0761, Cost 0.3579 sec
07-25 16:45:34 -----Epoch 1/255-----
07-25 16:45:34 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
07-25 16:45:39 Epoch: 1 source_train-Loss: 13459.7950, recon_loss:11610.4854, kl_loss:2.1605, Cost 4.8596 sec
07-25 16:45:39 Epoch: 1 source_val-Loss: 10664.5780, recon_loss:10546.0918, kl_loss:2.0962, Cost 0.3951 sec
07-25 16:45:39 -----Epoch 2/255-----
07-25 16:45:39 current lr: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
07-25 16:45:44 Epoch: 2 source_train-Loss: 10484.2598, recon_loss:10299.8545, kl_loss:2.6024, Cost 5.0230 sec
Traceback (most recent call last):
  File "E:\Galaxy\yang7hi_battery\train_ae.py", line 135, in <module>
    trainer.train(cond=condition)
  File "E:\Galaxy\yang7hi_battery\utlis\train_utils_ae.py", line 362, in train
    x = self.encoder(inputs)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "E:\Galaxy\yang7hi_battery\model\VARE.py", line 59, in forward
    attention_score = self.att_net(h_end)           # torch.Size([128, 1])
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\container.py", line 139, in forward
    input = module(input)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt