08-31 16:02:47 -----Epoch 0/99-----
08-31 16:02:47 current lr: [0.001, 0.001, 0.001]
08-31 16:02:48 Epoch: 0 [0/6752], Train Loss: 1.0881 Train Acc: 0.3164,19.7 examples/sec 12.98 sec/batch
08-31 16:02:52 Epoch: 0 source_train-Loss: 0.9865 source_train-Acc: 0.5222, Cost 4.6 sec
08-31 16:02:53 Epoch: 0 source_val-Loss: 0.5566 source_val-Acc: 0.8817, Cost 1.7 sec
08-31 16:02:54 Epoch: 0 target_val-Loss: 0.8302 target_val-Acc: 0.7361, Cost 0.9 sec
08-31 16:02:54 -----Epoch 1/99-----
08-31 16:02:54 current lr: [0.001, 0.001, 0.001]
Isc recall: 56.64944444444445
08-31 16:02:56 Epoch: 1 source_train-Loss: 0.4264 source_train-Acc: 0.9304, Cost 2.0 sec
08-31 16:02:57 Epoch: 1 source_val-Loss: 0.3636 source_val-Acc: 0.9542, Cost 0.3 sec
08-31 16:02:57 Epoch: 1 target_val-Loss: 0.6402 target_val-Acc: 0.8008, Cost 0.4 sec
08-31 16:02:57 -----Epoch 2/99-----
08-31 16:02:57 current lr: [0.001, 0.001, 0.001]
Isc recall: 53.66
08-31 16:02:59 Epoch: 2 source_train-Loss: 0.3337 source_train-Acc: 0.9665, Cost 2.0 sec
Isc recall: 88.1
08-31 16:02:59 Epoch: 2 source_val-Loss: 0.2999 source_val-Acc: 0.9755, Cost 0.2 sec
08-31 16:03:00 Epoch: 2 target_val-Loss: 0.7351 target_val-Acc: 0.7869, Cost 0.4 sec
08-31 16:03:00 -----Epoch 3/99-----
08-31 16:03:00 current lr: [0.001, 0.001, 0.001]
08-31 16:03:02 Epoch: 3 source_train-Loss: 0.3134 source_train-Acc: 0.9743, Cost 2.0 sec
08-31 16:03:02 Epoch: 3 source_val-Loss: 0.2933 source_val-Acc: 0.9801, Cost 0.2 sec
08-31 16:03:02 Epoch: 3 target_val-Loss: 0.6831 target_val-Acc: 0.8045, Cost 0.4 sec
08-31 16:03:02 -----Epoch 4/99-----
08-31 16:03:02 current lr: [0.001, 0.001, 0.001]
Traceback (most recent call last):
  File "train_base.py", line 128, in <module>
    trainer.train(cond=condition)
  File "E:\Galaxy\yang7hi_battery\utlis\train_utils_base.py", line 360, in train
    features = self.model(inputs)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "E:\Galaxy\yang7hi_battery\model\ALstm.py", line 125, in forward
    attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1] 计算注意力分数
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\container.py", line 139, in forward
    input = module(input)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
  File "D:\anaconda\envs\pytorch\lib\traceback.py", line 193, in format_stack
    def format_stack(f=None, limit=None):
KeyboardInterrupt
Isc recall: 92.775